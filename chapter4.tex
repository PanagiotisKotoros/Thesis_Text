\chapter{Κατηγοριοποίηση Δεδομένων}
	\section{Μηχανές Διανυσμάτων Υποστήριξης ({\en{Support Vector Machines}})}
			\subsection{Εισαγωγή}
		\par Οι Μηχανές Διανυσμάτων Υποστήριξης ({\en{Support Vector Machines - SVM}}) αποτελούν αλγόριθμο κατηγοριοποίησης και ταξινόμησης στην κατηγορία της επιβλεπόμενης μάθησης, του τομέα της μηχανικής μάθησης. Η κύρια χρήση του αλγόριθμου σήμερα είναι σε προβλήματα ταξινόμησης και οι επιδόσεις του είναι αρκετά ικανοποιητικές.
			\par Ξεκινώντας, οι Μηχανές Διανυσμάτων Υποστήριξης κατά ένα μεγάλο ποσοστό, όπως προαναφέραμε, ασχολούνται με το κομμάτι της ταξινόμησης. Αλλά πώς ακριβώς δουλεύουν$;$ Αρχικά πρέπει να καταλάβουμε πώς ακριβώς δουλεύει η ανάλυση ταξινόμησης Θα δώσουμε ένα πολύ απλουστευμένο παράδειγμα ενός προβλήματος της ανάλυσης ταξινόμησης Έστω ότι θέλουμε να ταξινομήσουμε ένα τυχαίο δείγμα ανθρώπων σε άνδρες και γυναίκες. Σκοπός μας στην προκειμένη περίπτωση είναι να δημιουργήσουμε ένα πρόγραμμα με το οποίο ένα ρομπότ ή μία μηχανή θα μπορούσε  να εκτιμήσει το φύλο κάθε ατόμου στο δείγμα αυτό. Αρχικά, πρέπει να θέσουμε κάποιους κανόνες σύμφωνα με τους οποίους θα ξεχωρίζουμε τους άνδρες από τις γυναίκες με βάση τα εξωτερικά τους χαρακτηριστικά Υποθέτουμε ότι κατά μέσο όρο οι άνδρες έχουν μεγαλύτερο ύψος από τις γυναίκες και επίσης ότι κατά μέσο όρο οι γυναίκες έχουν μεγαλύτερα σε μέγεθος μαλλιά από τους άνδρες. Σε ένα τυχαίο δείγμα έξι ατόμων των οποίων τα χαρακτηριστικά φαίνονται στο σχήμα \ref{graph1} μπορούμε να δούμε το ύψος και το μέγεθος των μαλλιών σε εκατοστά κάθε ατόμου. Εύκολα μπορεί κανείς να παρατηρήσει ότι οι γυναίκες, οι οποίες είναι μαρκαρισμένες με το κόκκινο χρώμα, είναι σε διαφορετικό κομμάτι σε σχέση με τους άνδρες, που είναι με το μπλε χρώμα. Στο πρόβλημα αυτό έχουμε ένα καλά διαχωρισμένο δείγμα με αποτέλεσμα να έχουν δημιουργηθεί δύο διαφορετικές περιοχές, μία που περιέχει άνδρες και μία που περιέχει γυναίκες, πράγμα που κάνει τη δουλειά της ταξινόμησης πολύ ευκολότερη.\begin{figure}
			\begin{center}
			    \includegraphics[width=0.8\linewidth]{grafpara.eps}
			    \caption{Αναπαράσταση Δείγματος}
			    \label{graph1}
			    \end{center}
			\end{figure}
			\par Το παραπάνω πρόβλημα αποτελεί ένα απλουστευμένο πρόβλημα του τομέα της ανάλυσης και ταξινόμησης δεδομένων. Προκειμένου να φτάσουμε σε μία μέθοδο η οποία θα μας επιλύει απλά προβλήματα όπως το προαναφερθέν, δυσκολότερα ή ακόμα και πολύ σύνθετα πολύπλοκα προβλήματα πρέπει αρχικά να κατανοήσουμε τι προσπαθούμε να επιτύχουμε. Όπως μπορούμε να δούμε στο σχήμα \ref{graph1} το δείγμα έχει χωριστεί σε δύο ομάδες. Αυτό που θέλουμε είναι να βρούμε μία μέθοδο η οποία θα πετύχει την ταξινόμηση του δείγματος μου σε άνδρες και γυναίκες με βάση τους κανόνες που θέσαμε. Για αρχή παρατηρούμε ότι κάθε άτομο στο παραπάνω δείγμα αποτελεί και ένα σημείο του χώρου που ορίζει η γραφική παράσταση, η οποία απεικονίζει το κάθε άτομο μέσα στο δείγμα ως ένα σημείο συναρτήσει του ύψους του σε εκατοστά (άξονας $xx'$) και του μεγέθους, σε μήκος μετρημένο σε εκατοστά, των μαλλιών του (άξονας $yy'$). Ο σκοπός μας είναι να βρούμε μια γραμμή η οποία θα διαχωρίσει το δείγμα. Δηλαδή ψάχνουμε εκείνη την ευθεία για την οποία ισχύει ότι κάθε σημείο πάνω από αυτή θα είναι γυναίκα ενώ κάθε σημείο κάτω από αυτή θα είναι άνδρας.
			\begin{figure}
			\begin{center}
			    \includegraphics[width=0.8\linewidth]{grafpara_2.eps}
			    \caption{Υπερεπίπεδα}
			    \label{graph2}
			    \end{center}
			\end{figure}
		\par	Η γραμμή-όριο στην οποία έγινε αναφορά ονομάζεται όριο απόφασης ({\en{decision boundary}}) στον τομέα των Μηχανών Διανυσμάτων Υποστήριξης ({\en{SVM}}). Στο συγκεκριμένο παράδειγμα είναι πολύ εύκολο να βρεθεί όριο απόφασης και μάλιστα υπάρχουν αρκετές λύσεις οι οποίες ικανοποιούν τη συνθήκη που θέσαμε στην αρχή. Το γεγονός αυτό γεννά την απορία του αν υπάρχει σωστό ή λάθος όριο και σε περίπτωση που υπάρχει, ποιο είναι το κριτήριο επιλογής. Στο σχήμα \ref{graph2} μπορούμε να διακρίνουμε κάποιες γραμμές καθεμία από τις οποίες διαχωρίζει πλήρως τα δύο μας δείγματα. Η σωστή επιλογή ενός ορίου απόφασης γίνεται με βάση το {\en{support vector}} του εκάστοτε δείγματος. Ο {\en{support vector}} αποτελείται από $n+1$ στοιχεία, όπου $n$ η διάσταση του πίνακα του δείγματος μας, τα οποία βρίσκονται πιο κοντά στο κενό το οποίο χωρίζει τα δύο δείγματα. Στο παράδειγμα μας, ο {\en{support vector}}, θα αποτελείται από τα τρία κοντινότερα σημεία στα πιθανά όρια $A,B$ και $C$ του σχήματος \ref{graph2}. Για να βρούμε το σωστό {\en{decision boundary}} ψάχνουμε για την ευθεία εκείνη που χωρίζει όλα τα σημεία του δείγματος μας έτσι ώστε να έχουν τη μεγαλύτερη δυνατή απόσταση από το όριο απόφασης. Παρακάτω θα δούμε ότι αντί να εξεταστεί η απόσταση κάθε σημείου από το όριο απόφασης αρκεί να εξετάσουμε τις αποστάσεις ως προς τα σημεία του {\en{support vector}}. Η απλότητα του συγκεκριμένου προβλήματος, δεν καθιστά προφανή τον λόγο επιλογής του ορίου με βάση την απόσταση του από τα σημεία αυτά. Σε μεγαλύτερα δείγματα όπου τα σημεία έρχονται πολύ πιο κοντά το ένα στο άλλο και πιθανότατα πιο κοντά στο όριο απόφασης, όσο πιο μεγάλη είναι η απόσταση, τόσο μικρότερο το σφάλμα γενίκευσης του ταξινομητή.  
		
		
		\subsection{Μαθηματική Ανάλυση των Μηχανών Διανυσμάτων Υποστήριξης ({\en{SVM}})}
		Το πρώτο βήμα για να κατανοήσουμε την μαθηματική πλευρά των Μηχανών Διανυσμάτων Υποστήριξης ({\en{SVM}}) είναι να καταλάβουμε την έννοια τις απόστασης ({\en{margin}}). Έστω $x$ τα εισαγόμενα δεδομένα,  $y$ τα δεδομένα εκπαίδευσης και $\theta$ βάρος. Έστω ένα πρόβλημα παλινδρόμησης, στο οποίο η πιθανότητα $p(y=1| x; \theta)$ διαμορφώνεται από την σχέση $h^\theta (x) = g(\theta^T x)$. Ένα στοιχείο $x$ θα έχει σαν αποτέλεσμα $'1'$ αν και  μόνο αν $h^ \theta (x) \geq 0.5$ ή ισοδύναμα αν και μόνο αν $ \theta ^T x \geq 0$. Αν πάρουμε σαν παράδειγμα ένα θετικό στοιχείο δείγματος ($y=1$), τότε όσο πιο πολύ μεγαλώνει η ποσότητα $\theta^T x$ απολύτως ανάλογα μεγαλώνει και το  $h^ \theta (x)$ και συνεπώς μεγαλώνει ο βαθμός βεβαιότητας μας για το αποτέλεσμα. Ανάλογα μπορούμε να συλλογιστούμε ότι για να προκύψει το αποτέλεσμα $y=0$ θα πρέπει οι ποσότητες αυτές να είναι αρκετά μεγάλοι αρνητικοί αριθμοί. Δηλαδή θέλουμε $\theta^T x<<0$. Γενικά, η μέθοδος φαίνεται να λειτουργεί αν καταφέρουμε να βρούμε $\theta$ τέτοιο ώστε η ποσότητα $\theta^T x$ θα είναι πολύ μεγαλύτερη του μηδενός όταν θα έχω αποτέλεσμα $y^{(i)}=1$ και αντίστοιχα πολύ μικρότερη του μηδέν αν το αποτέλεσμα είναι $y^{(i)}=0$. Έχοντας αυτή τη βασική ιδέα συνεχίζουμε χρησιμοποιώντας τα λειτουργικά περιθώρια ({\en{functional margins}}).
		\par Στο παρακάτω σχήμα \ref{abc} έχουμε κάποια $+$ τα οποία αναπαριστούν θετικά στοιχεία εκπαίδευσης και κάποια $-$ τα οποία θα είναι αρνητικά στοιχεία εκπαίδευσης. Επίσης έχουμε μια γραμμή απόφασης (η οποία προκύπτει $\theta^T x=0$ και ονομάζεται γραμμή διαχωρισμού) και τρία σημεία $A, B$ και $C$. 
		\par Παρατηρώντας το σημείο $A$ αν μας ζητηθεί να προβλέψουμε σε ποία από τις δύο κατηγορίες ανήκει (όπου $y=1$ τα θετικά και $y=0$ τα αρνητικά) είναι πολύ λογικό να μαντέψουμε ότι είναι θετικό. Αντίστροφα, το σημείο $C$ είναι πολύ κοντά στην διαχωριστική γραμμή. Παρόλο που μπορούμε να δούμε ότι είναι στη μεριά των θετικών στοιχείων, με μία πολύ μικρή αλλαγή της διαχωριστικής γραμμής μπορεί να βρεθεί στην αρνητική μεριά. Το σημείο $B$ βρίσκεται ενδιάμεσα στις δύο αυτές, σχεδόν ακραίες, περιπτώσεις και μας δείχνει ότι αν ένα στοιχείο έχει απόσταση από τη διαχωριστική γραμμή κάνει τη δουλειά της κατηγοριοποίησης πολύ πιο απλή. Ο στόχος μας λοιπόν είναι να καταφέρουμε να διαχωρίσουμε το δείγμα με μεγάλο βαθμό βεβαιότητας και σιγουριάς.
		
		\begin{figure}
			\begin{center}
			    \includegraphics[width=0.6\linewidth]{line_role.eps}
			    \caption{Ρόλος Γραμμής Απόφασης}
			    \label{abc}
			    \end{center}
			\end{figure}
		
	\subsection{Χρήσιμα Σύμβολα}
	 Για αρχή θα χρειαστεί να αναφερθούμε σε μία νέα σημειογραφία προκειμένου να διευκολύνουμε τη διαδικασία της απόδειξης. Αρχικά, θεωρούμε μία γραμμική συνάρτηση κατηγοριοποίησης η οποία θα απευθύνεται σε προβλήματα δυαδικής κατηγοριοποίησης. Έχουμε ταμπέλες $y$ που χαρακτηρίζονται από χαρακτηριστικά $x$ η καθεμία. Αντί για πίνακα $\theta$ όπως είδαμε παραπάνω, θα χρησιμοποιήσουμε παραμέτρους $w,b$. Η συνάρτηση κατηγοριοποίησης θα είναι: \\
	$$h_{w,b}(x)=g(w^T x + b)$$ 
	Η συνάρτηση μας θα δίνει $g(z)=1$ αν $z \geq 0 $
ενώ σε κάθε άλλη περίπτωση θα έχουμε $g(z)=-1$. Κάνοντας χρήση των $w,b$ έχουμε το πλεονέκτημα να μπορούμε να αντιμετωπίσουμε τον όρο $b$ χωρίς εξάρτηση από τις υπόλοιπες παραμέτρους. Πρακτικά το $b$ που έχουμε εδώ αντικαθιστά το $\theta _0$ της προηγούμενης παραγράφου, ενώ το $w$ παίρνει τη θέση του πίνακα $[\theta_1, \theta_2, ..., \theta_n]^T$
	\par Σε αυτό το σημείο μπορούμε εύκολα να παρατηρήσουμε ότι ο τρόπος με τον οποίο ορίσαμε τη συνάρτηση $g$ μπορεί να δώσει δύο πιθανά αποτελέσματα. Συνεπώς η διαδικασία που θα ακολουθηθεί θα είναι ένας έλεγχος για τον αν το αποτέλεσμα θα είναι η πρώτη περίπτωση ($z \geq 0$), αλλιώς θα μεταφερθούμε αυτόματα και χωρίς άλλο έλεγχο στη δεύτερη περίπτωση. Η μέθοδος αυτή διαφοροποιείται από εκείνη της λογιστικής παλινδρόμησης.% και μας θυμίσει αρκετά τον τρόπο λειτουργίας των δίκτυων {\en{perceptrons}}.
	\subsection{Ορισμός Αποστάσεων ({\en{Margins}})}
	 Σε αυτό το σημείο θα ορίσουμε τις αποστάσεις. Δεδομένου πάντα ενός δείγματος $(x^{(i)},y^{(i)})$. Το {\en{functional margin}} των $w,b$  ως προς το δείγμα μας θα είναι:\\
	$$γ^{(i)}=y^{(i)}(w^T x +b)$$
	Σε περίπτωση που το $y^(i)=1$, τότε προκειμένου να έχουμε μεγαλύτερη απόσταση, και συνεπώς η πρόβλεψη μας να είναι πιο βέβαιη και σωστή, θέλουμε το $w^Tx+b$ να είναι ένας μεγάλος θετικός αριθμός. Με απόλυτη αναλογία στην περίπτωση όπου $y^{\left( i \right)} =-1$ για να πετύχουμε μεγαλύτερη απόσταση, θέλουμε το $w^Tx+b$ να είναι ένας μεγάλος αρνητικός αριθμός. Είναι εύκολο να παρατηρήσουμε ότι αν η ποσότητα $y^{(i)}(w^T x+b)>0$ τότε το πόρισμα που βγάλαμε για το συγκεκριμένο παράδειγμα είναι σωστή. 
	\par Ωστόσο στο {\en{functional margin}} υπάρχει ένα πρόβλημα. Για μία γραμμική συνάρτηση $g$ η οποία λαμβάνει τιμές από το σύνολο ${-1, 1}$ αν αντικαταστήσουμε τα $w, b$ με $2w, 2b$ τότε προκύπτει:\\
	 $$g(2w^T x+2b) $$
	Το αποτέλεσμα δεν επηρεάζεται άμεσα μιας και έχουμε δυαδική συνάρτηση. Οι $h_{w,b}$ και $g$ δε θα επηρεαστούν, και αυτό διότι εξαρτώνται από τα πιθανά αποτελέσματα και όχι από την αξία της ποσότητας $w^t x+b$. Όμως, η αντικατάσταση των $w, b$ με $2w, 2b$ είναι πρακτικά ο πολλαπλασιασμός της ήδη υπάρχουσας απόστασης μας με τον αριθμό δύο. Μπορούμε πολύ εύκολα να παρατηρήσουμε ότι κάνοντας χρήση της παραπάνω σκέψης είναι δυνατό να μεγαλώσουμε την απόσταση αυτή απειροστά, χωρίς να έχουμε να προσφέρουμε κάποια ουσιαστική λύση στο πρόβλημα. Ένας τρόπος να γλυτώσουμε από το πρόβλημα αυτό είναι η κανονικοποίηση της απόστασης. 
	\par Δεδομένου ενός δείγματος εκπαίδευσης $S={(x^{(i)},y^{(i)}):i=1,...,m}$, επίσης ορίζουμε την απόσταση της συνάρτησης των $(w,b)$  ως προς $S$, ως τη μικρότερη από τις αποστάσεις κάθε στοιχείου στο δείγμα μας. Αν το συμβολίσουμε με $\gamma$ (σχήμα \ref{kath}) τότε θα ισούται με:\\
	$$\gamma=min_{i=1,...,m} \gamma_{(i)}$$
	
            \begin{figure}
			   \begin{center}
			    	\includegraphics[width=0.6\linewidth]{hyper.eps}
			    	\caption{Υπερεπίπεδα}
			    	\label{kath}
			    \end{center}
			\end{figure}	
	
\par Συνεχίζοντας, πρέπει να βρούμε ένα τρόπο να υπολογίσουμε το $\gamma_{(i)}$. Σύμφωνα με το παραπάνω σχήμα \ref{kath} μπορούμε να δούμε ότι το $w/ \Vert w \Vert$ είναι ένα μοναδιαίο διάνυσμα ομόρροπο με το $w$ το οποίο είναι κάθετο στο υπερεπίπεδο. Αφού το $Α$ αναπαριστά το $x(i)$ τότε μπορούμε να πάρουμε το σημείο $Β$ από την αφαίρεση $x(i)-\gamma(i)$. Παρατηρούμε όμως ότι το συγκεκριμένο σημείο ανήκει στο υπερεπίπεδο απόφασης, και κάθε τέτοιο σημείο ικανοποιεί τη συνάρτηση $w^T x + b = 0 $.
Συνεπώς:\\
$$w^T \left( x^{(i)}-\gamma^{(i)} \dfrac{w}{\Vert w \Vert} \right) + b = 0 $$
και ως προς $\gamma^{(i)}$ έχουμε
$$ \gamma^{(i)}= \dfrac{w^T}{\Vert w \Vert} x^{(i)} + b = \dfrac{w^T}{\Vert w \Vert} x^{(i)}+ \dfrac{b}{\Vert w \Vert}  $$

Από την παραπάνω σχέση μπορούμε να ορίσουμε τη γεωμετρική απόσταση {\en{geometric margin}} των $(w, b)$ ως προς ένα δείγμα $(x^{(i)}, y^{(i)})$ να είναι:


$$\gamma ^{(i)}= y^{(i)} \left(   \dfrac{w^T}{\Vert w \Vert}  x^{(i)} + \dfrac{b}{\Vert w \Vert} \right) \label{eq1}  $$


Η γεωμετρική απόσταση που μόλις ορίσαμε μπορεί εύκολα να συσχετιστεί με την αρχική μας εκτίμηση. Αρκεί απλά να στη σχέση \ref{eq1} να πάρουμε $\Vert w \Vert = 1 $. Το πλεονέκτημα που κρύβει η νέα σχέση απόστασης που ορίσαμε είναι ότι επειδή είναι κανονικοποιημένη δεν παρουσιάζει πρόβλημα με την κλιμακοποίηση. Σε περίπτωση που αντικαταστήσουμε το $w$ με $2w$ η απόσταση θα παραμείνει ακριβώς ίδια. Με προσαρμογή στα νέα δεδομένα λοιπόν, δεδομένου δείγματος $S={(x^{(i)},y^{(i)}):i=1,...,m}$ η γεωμετρική απόσταση των $(w, b)$ ως προς το $S$ θα είναι η μικρότερη πιθανή απόσταση από καθεμία από τις ξεχωριστές αποστάσεις κάθε στοιχείου στο δείγμα:\\
$$\gamma=min_{i=1,..,m} \gamma^{(i)} $$

\subsection{Βελτιστοποίηση}
Μπορούμε λοιπόν να δούμε ότι μία λύση με με μεγάλη ακρίβεια και βαθμό βεβαιότητας είναι η μεγιστοποίηση της γεωμετρικής απόστασης. Σαν γεωμετρική εξήγηση, θέλουμε να βρούμε γραμμή όριο η οποία θα έχει το μεγαλύτερο δυνατό κενό από τα κοντινότερα σε εκείνη στοιχεία(και προφανώς θα τα διαχωρίζει). 
\par Αρχικά θεωρούμε ότι δείγμα μας είναι γραμμικό, και συνεπώς είναι εφικτό να διαχωριστεί με μία γραμμή. Σκοπός μας είναι η λύση του προβλήματος βελτιστοποίησης:\\
$$max_{\gamma,w,b} \gamma$$\\
τέτοιο ώστε$$y^{(i)}(w^T x^{(i)}+b) \geq \gamma , i=1,...,m ,\Vert w \Vert =1  $$

Ουσιαστικά, θέλουμε να μεγιστοποιήσουμε το $\gamma$ σε σχέση με κάθε στοιχείο στο δείγμα, του οποίου η απόσταση είναι τουλάχιστον $\gamma$. Αφού όλες οι γεωμετρικές αποστάσεις είναι τουλάχιστον $\gamma$, η λύση αυτού το προβλήματος βελτιστοποίησης θα μου δώσει το ζεύγος $(w, b)$ που επιτυγχάνει τη μέγιστη δυνατή απόσταση. Το βασικό πρόβλημα που αντιμετωπίζουμε είναι η συνθήκη $\Vert w \Vert =1$, η οποία υπάρχει για τη συσχέτιση των δύο αποστάσεων που ορίστηκαν στην προηγούμενη υποενότητα. Αν η λύση του προβλήματος ήταν εφικτή στη δεδομένη μορφή, η απόδειξη θα τελείωνε εδώ. Δυστυχώς όμως η επίλυση γίνεται αρκετά δύσκολη από τη συνθήκη $\Vert w \Vert  =1$, η οποία κάνει το πρόβλημα μη-κυρτό γραμμικό πρόβλημα. Όντας δύσκολο να ανιχνεύσουμε τη λύση βελτιστοποίησης ακόμα και με κάποιο υπολογιστικό πρόγραμμα, πρέπει να τροποποιήσουμε τη σχέση για να βρούμε λύση.
\par Αρχικά, μπορούμε να μετατρέψουμε το $\gamma$ με βάση τον τύπο $\gamma = \dfrac{\hat{\gamma}}{\Vert w \Vert}$ ο οποίος σχετίζει τις δύο αποστάσεις (γεωμετρική, πρακτική) ως εξής (με $\hat{\gamma}$ συμβολίζουμε τη νέα απόσταση):\\
$$max_{\gamma,w,b}\dfrac{\hat{\gamma}}{\Vert w\Vert}$$
τέτοιο ώστε $$y^{(i)}(w^T x^{(i)}+b) \geq \hat{ \gamma} , i=1,...,m $$

Συνεπώς, το πρόβλημα τώρα είναι η μεγιστοποίηση της ποσότητας $ \dfrac{\hat{\gamma}}{\Vert w\Vert}$ ως προς τις αποστάσεις που είναι τουλάχιστον $\hat{\gamma}$. Έχοντας πλέον απαλλαγεί από το πρόβλημα της συνθήκης $\Vert w \Vert =1$ συνεχίζουμε, αλλά αρκετά γρήγορα συνειδητοποιούμε, ότι αντιμετωπίζομε ακριβώς το ίδιο πρόβλημα και αυτή τη φορά με το $\Vert w \Vert$ στον παρονομαστή της σχέσης που θέλουμε να μεγιστοποιήσουμε.
\par Αυτή τη φορά θα κάνουμε χρήση ενός συμπεράσματος που είδαμε παραπάνω. Κατά το συμπέρασμα αυτό είδαμε ότι η μεταβολή των $(w, b)$ κλιμακωτά στη γεωμετρική απόσταση, δεν επηρεάζει καθόλου την απόσταση. Εισάγουμε λοιπόν αρχικά μία συνθήκη, ότι η πρακτική απόσταση των $(w, b)$ ως προς δείγμα μας πρέπει να είναι ένα ($\hat{\gamma}=1$).
\par Οπότε, με το νέο περιορισμό, μπορούμε να μετατρέψουμε τη σχέση $\dfrac{\hat{\gamma}}{\Vert w \Vert}$ σε $\dfrac{1}{\Vert w \Vert}$. Χωρίς βλάβη της γενικότητας μπορώ να μετατρέψω την τελευταία σχέση σε $\dfrac{1}{2}\Vert w \Vert$ και τελικά, να καταλήξω στο πρόβλημα:\\
$$min_{w,b} \dfrac{1}{2}\Vert w \Vert $$\\
τέτοιο ώστε $$y^{(i)}(w^T x^{(i)}+b) \geq \hat{\gamma} , i=1,...,m $$ 
\par Τελικά, μετατρέψαμε το πρόβλημα σε εύκολα επιλύσιμη μορφή από κάποιο πρόγραμμα επίλυσης κυρτών τετραγωνικών προβλημάτων με γραμμικές συνθήκες. Η λύση της κυρτής τετραγωνικής μορφής που προέκυψε στο τελικό βήμα, θα μας δώσει τον βέλτιστο ταξινομητή.
\par Ενώ η απόδειξη τελειώνει εδώ, μπορούμε να συνεχίσουμε και να επεκταθούμε στη δυϊκή μορφή {\en{Lagrange}} του προβλήματός μας. Έτσι θα μπορέσουμε να αναφερθούμε στους πυρήνες, οι οποίοι παίζουν καθοριστικό ρόλο στην επίλυση πολυδιάστατων προβλημάτων. Ακόμη θα βοηθήσει να επιλύσουμε ακόμη πιο αποδοτικά το παραπάνω πρόβλημα από τις μεθόδους του γραμμικού προγραμματισμού.

\subsection{Χρήση Μεθόδου {\en{Lagrange}}}
	
	Αρχικά, αντιμετωπίζουμε το πρόβλημα από μαθηματικής πλευράς. Έστω ότι είναι της μορφής:\\
	$$min_w f(w)$$\\
	τέτοιο ώστε $$h_i(w)=0, i=1,...,m$$\\
	Η μέθοδος {\en{Lagrange }} για την επίλυση του παραπάνω προβλήματος θα μας δώσει τη σχέση:\\
	\begin{align}
		L(w,a,b)= & f(w)+ \sum_{i=1}^I b_i h_i (w) \label{eq2} 
	\end{align}
	
	όπου $b_i$ οι πολλαπλασιαστές {\en{Lagrange}}. Έπειτα παίρνουμε τις μερικές παραγώγους της σχέσης \ref{eq2} ίσες με μηδέν και λύνουμε ως προς $w$ και $b$:\\
	\begin{align}
		\dfrac{\partial L}{\partial {w_i}}= 0 ,\hspace{5mm} \dfrac{\partial L}{\partial {b_i}}= 0
	\end{align}
	
	Στη συνέχεια θα ασχοληθούμε με τις ιδέες και τα αποτελέσματα που θα μας οδηγήσουν στη βελτιστοποίηση του αλγόριθμου κατηγοριοποίησης. Το πρόβλημά μας σε μια πρώιμη μορφή είναι:\\
	\begin{align}
			g_i(w) \leq 0, i=1,..,k\\
			h_i(w) = 0, i=1,..,l
	\end{align}
	Ο γενικευμένος τύπος {\en{Lagrange}} λοιπόν θα είναι:\\
	\begin{align}
		L(w,a,b)= f(w)+ \sum_{i=1}^k a_ig_i (w)+ \sum_{i=1}^^l b_ih_i(w)
	\end{align}
	όπου $a_i$ και $b_i$ πολλαπλασιαστές {\en{Lagrange}}. Θέτουμε $\theta(w)=max_{a,b a_i\geq 0} L(w,a,b)$
	Σε περίπτωση που η παραπάνω ποσότητα λάβει $w$ που παραβιάζει κάποιον από τους αρχικούς περιορισμούς τότε η σχέση θα γίνει:\\
	$$\theta(w)=max_{a,b:a_i\geq 0} f(w) + \sum_{i=1}^k a_i g_i(w)+ \sum_{i=1}^l b_i h_i (w)$$ %= \infty $$
	Αν οι περιορισμοί ικανοποιούνται για κάποια τιμή του $w$ τότε $$\theta(w)=f(w)$$ Συνεπώς $\theta(w)= f(w)$ αν το $w$ ικανοποιεί τις αρχικές συνθήκες, ενώ θα είναι ίσο με άπειρο σε κάθε άλλη περίπτωση.  
	Άρα, το πρόβλημα μεταφράζεται ως:\\
		$$min_w \theta(w)=min_w max_{a,b :a_i\geq 0} L(w,a,b) $$
	Μέχρι στιγμής η διατύπωση δεν έχει ουσιαστικές αλλαγές. Στο σημείο αυτό θα εξετάσουμε μία διαφορετική προσέγγιση:\\
	$$\theta_D(a,b)=min_w L(w,a,b)$$
	Ο δείκτης $D$ υπάρχει για να δείχνει τη διπλή συνθήκη . Έχοντας ορίσει το $\theta_0$, τώρα θέλουμε να βρούμε:\\
	$$max_{a,b:a_i \leq 0}\theta_d (a,b)=max_{a,b:a_i \leq 0} min_w L(w,a,b) $$
	Το παρόν πρόβλημα διαφέρει από το αρχικό στο σημείο όπου τα {\en{min}} και {\en{max}} έχουν ανταλλάξει θέσεις. Ορίζουμε $d^*=max_{a,b:a_i \leq 0}\theta_D(w)$. Τότε θα ισχύει:\\
	\begin{align}
		d^*=max_{a,b:a_i \leq 0} min_w L(w,a,b) \leq min_w max_{a,b:a_i \leq 0} L(w,a,b)=p^*
	\end{align}
	Υπό συγκεκριμένες συνθήκες μπορεί να προκύψει ισότητα ανάμεσα στα $d^*$ και $p^*$. Οι συνθήκες αυτές είναι οι εξής:\\
	\par Έστω $f,g$ κυρτές συναρτήσεις και $h_i$ γραμμική. Επίσης υποθέτουμε ότι οι περιορισμοί $g_i$ είναι εφικτοί. Τότε θα υπάρχει $w$ τέτοιο ώστε $g_i(w)<0$ για κάθε $i$. Σύμφωνα με τα παραπάνω, θα πρέπει να υπάρχουν $w^*,a^*,b^*$ τέτοια ώστε το $w^*$ να είναι η λύση του αρχικού μας προβλήματος και  τα  $a^*,b^*$ να είναι λύσεις του δυϊκού προβλήματος. Επιπροσθέτως, θα έχουμε $p^*=d^*=L(w^*,a^*,b^*)$ και τα $w^*,a^*b^*$ θα ικανοποιούν τις συνθήκες {\en{Karush-Kuhn-Tucker (KKT)}} οι οποίες είναι:\\
\begin{align}
	\dfrac{\partial}{\partial{w_i}}L(w^*,a^*,b^*) &=0,\hspace{5mm} i=1,..,n\\
	\dfrac{\partial}{\partial{b_i}}L(w^*,a^*,b^*) &= 0, \quad i=1,...,l\\
	a_i ^* g_i(w^*) &=0, \quad i=1,...,k \label{eq10}\\
	g_i(w^*)  &\leq 0, \quad i=1,...,k\\
	a^* &\geq 0, \quad i=1,..,k
\end{align}	

Επιπρόσθετα, αν κάποια από τα $w^*,a^*,b^*$ ικανοποιούν τις συνθήκες {\en{KKT}} τότε αποτελούν λύση και για το αρχικό και για το δυικό πρόβλημα.\\
\par Η προσοχή μας θα στραφεί στη συνθήκη \ref{eq10} η οποία είναι γνωστή και ως δυϊκή συμπληρωματική συνθήκη των {\en{KKT}}. Πιο συγκεκριμένα μας δείχνει ότι αν έχουμε $a_i ^*>0$, τότε $g_i(w^*)=0$
	 
	
	\subsection{Βελτιστοποίηση}
	Στην πέμπτη ενότητα καταλήξαμε στην εξής διατύπωση για το πρόβλημα:\\
	$$min_{w,b} \dfrac{1}{2} \Vert w \Vert ^2$$\\
	τέτοιο ώστε $$y_{(i)}(w^Tx^{(i)}+b) \geq 1, i=1,..,m$$
Οι περιορισμοί μπορούν να γραφούν επίσης με τη μορφή $g_i(w)=y^{(i)}(w^Tx^{(i)}+b)+1 \leq 0$ και υπάρχει ένας αντίστοιχος περιορισμός για κάθε στοιχείο στο δείγμα μας. Λόγω της συνθήκης διπλής συμπληρωματικότητας των ({\en{KKT}}), θα έχουμε $a_i>0$ μόνο για τα στοιχεία που έχουν πρακτική απόσταση ακριβώς ίση με ένα. Έστω ότι έχουμε το παρακάτω σχήμα στο οποίο η συνεχής γραμμή είναι το βέλτιστο {\en{hyperplane}}. Τα σημεία με τη μικρότερη απόσταση είναι εκείνα που βρίσκονται κοντινότερα στη γραμμή απόφασης. Στο σχήμα μας αναφερόμαστε στα τρία μαρκαρισμένα στοιχεία(ένα αρνητικό και δύο θετικά) που βρίσκονται πάνω στις διακεκομμένες γραμμές. Τα τρία αυτά σημεία $a_i, i=1,2,3$ θα είναι τα μη μηδενικά σημεία τα οποία θα υπολογίσουμε στις ακόλουθες πράξεις και είναι οι {\en{support vectors}} του προβλήματός μας. Στη συνέχεια θα δούμε γιατί ο περιορισμός των στοιχείων που καλούμαστε να υπολογίσουμε με τη χρήση των {\en{support vectors}} είναι τόσο σημαντικός.
	\par Συνεχίζοντας την απόδειξη, θα προσπαθήσουμε να εκφράσουμε τον αλγόριθμο μας σαν όρους εσωτερικού γινομένου ανάμεσα στα στοιχεία που του εισάγουμε. Το βήμα αυτό θα φανεί πολύ χρήσιμο κατά την ανάλυση της μεθόδου του πυρήνα. Η κατασκευή του τύπου {\en{Lagrange}} του προβλήματος ήταν:\\
	\begin{align}
		L(w,a,b)=\dfrac{1}{2} \Vert w \Vert ^2 -\sum_{i=1}^m a_i(y^{(i)}(w^Tx^{(i)}+b) \label{eq3}
	\end{align}		
	\par Για να μεταφραστεί η παραπάνω σχέση σε διπλό πρόβλημα πρέπει πρώτα να ελαχιστοποιήσουμε το $L(w,a,b)$ ως προς $w$ και $b$ (για εάν δοσμένο $a$). Έτσι θα βρούμε το $\theta_D$ το οποίο προκύπτει εξισώνοντας τις μερικές παραγώγους του $L$, ως προς $w$ και $b$, να είναι ίσες με μηδέν. :\\
	\begin{align}
		w=\sum_{i=1}^m a_iy^{(i)}x^{(i)} \label{eq4}
	\end{align}
	
	
	\par Και η μερική παράγωγος ως προς $b$ είναι: 
		\begin{align}
	\dfrac{\partial}{\partial b} L(w,a,b)= \sum_{i=1}^ma_iy^{(i)}=0 \label{eq5}
	\end{align}			
	Αν συνδυάσουμε τον ορισμό του $w$ με τη σχέση \ref{eq4} και το αποτέλεσμα με τη σχέση \ref{eq3} προκύπτει:\\
	$$L(w,a,b)= \sum_{i=1}^m a_i-\dfrac{1}{2}\sum_{i,j=1}^m y^{(i)}y^{(j)}a_i a_j (x^{(i)})^Tx^{(j)}-b\sum_{i=1}^m a_i y^{(i)}$$
	και λόγω της σχέσης \ref{eq5} ο τελευταίος όρος είναι μηδέν, άρα:\\
	$$L(w,a,b)= \sum_{i=1}^m a_i-\dfrac{1}{2}\sum_{i,j=1}^m y^{(i)}y^{(j)}a_i a_j (x^{(i)})^Tx^{(j)}$$
	
	\par Δεδομένου ότι οι συνθήκες {\en{KKT}} ισχύουν και ότι $p^*=d^*$ αρκεί να λύσουμε το νέο διπλό πρόβλημα αντί του αρχικού. Ουσιαστικά είναι ένα πρόβλημα μεγιστοποίησης με παραμέτρους $a_i$. Θα ασχοληθούμε παρακάτω με την επίλυση του δ προβλήματος, αλλά σε περίπτωση που έχουμε τη λύση του, μπορούμε πολύ εύκολα από τη σχέση \ref{eq4} να βρούμε τα βέλτιστα $w$ με βάση τα $a$. Έπειτα έχοντας βρει το $w^*$ μπορούμε να βρούμε άμεσα το $b^*$ από τη σχέση:\\
	\begin{align}
		b^*=- \dfrac{max_{i:y^{(i)}=-1}w^{*T}x^{(i)}+min_{i:y^{(i)}=1}w^{*T}x^{(i)}}{2} \label{eq6}
	\end{align}	 
	
	Ας ασχοληθούμε λίγο παραπάνω με την εξίσωση \ref{eq4}. Υποθέτουμε ότι προσαρμόζουμε τις παραμέτρους του μοντέλου μας σε ένα σετ εκπαίδευσης και θέλουμε να προβλέψουμε το αποτέλεσμα σε μία νέα είσοδο $x$. Τότε θα έπρεπε να υπολογίσουμε την ποσότητα $w^T x+b$, και θα προβλέπαμε $y=1$ αν και μόνο αν η ποσότητα ήταν μεγαλύτερη του μηδέν. Από την σχέση \ref{eq4} η ποσότητα μπορεί να γραφεί:\\
	\begin{align}
		w^T x+b &= \left( \sum_{i=1}^m a_i y^{(i)} x^{(i)} \right)^T x+b \\
		&= \sum_{i=1}^m a_iy^{(i)} <x^{(i)},x>+b		
	\end{align}
	
	Συνεπώς, αν είχαμε βρει τα $a_i$, πριν κάνουμε πρόβλεψη, χρειάζεται να βρούμε μία ποσότητα που εξαρτάται μόνο από το εσωτερικό γινόμενο των $x$ με τα στοιχεία στο δείγμα. Επιπλέον, γνωρίζουμε ότι πολλά από τα $a_i$ θα είναι μηδενικά. Αυτή είναι η διευκόλυνση που δίνουν οι {\en{support vectors}}. Θα υπολογίσουμε λοιπόν  τα εσωτερικά γινόμενα για αυτά τα μη μηδενικά στοιχεία με το $x$ και έτσι θα προκύψει η πρόβλεψη μας.
	\par Εξετάζοντας την διπλή μορφή βελτιστοποίησης του αρχικού προβλήματος, κερδίσαμε σημαντικές γνώσεις ΄για τη δομή του προβλήματος. Ακόμη καταφέραμε να εκφράσουμε ολόκληρο τον αλγόριθμο με όρους εσωτερικού γινομένου μεταξύ των στοιχείων εισόδου. Στην ακόλουθη ενότητα θα εκμεταλλευτούμε την ιδιότητα αυτή για να εφαρμόσουμε τη μέθοδο του πυρήνα. Ο αλγόριθμος που θα προκύψει θα είναι ικανός να εκπαιδευτεί και να μάθει αποδοτικά σε μεγαλύτερες διαστάσεις.
	
	\subsection{Μέθοδος Πυρήνα}
	Πριν συνεχίσουμε με το μαθηματικό κομμάτι αξίζει να αναφέρουμε κάποια πράγματα για τη μέθοδο του πυρήνα. Ο λόγος ύπαρξης της μεθόδου του πυρήνα μπορεί να συνεισφέρει σε περιπτώσεις μη-γραμμικής διαχωρισιμότητας. Σε περίπτωση που δεν μπορούμε με κάποιο υπερεπίπεδο να διαχωρίσουμε το δείγμα μας προκειμένου διευκολυνθεί η διαδικασία της κατηγοριοποίησης, τότε η  μέθοδος του πυρήνα επιχειρεί να διαχωρίσει το δείγμα. Στην πράξη η διαδικασία η οποία ακολουθεί είναι να απεικονίσει το δείγμα σε ένα χώρο μεγαλύτερης διάστασης, στον οποίο η διαχωρισιμότητα είναι εφικτή, και έτσι να πετύχει το επιθυμητό αποτέλεσμα.
	\par Αρχικά πρέπει να διαχωρίσουμε κάποιες έννοιες. Θα αποκαλούμε αρχική είσοδο τα γνωρίσματα εισόδου ενός προβλήματος. Όταν αυτό αντιστοιχηθεί σε κάποιο νέο σύνολο ποσοτήτων που δίνουμε στον αλγόριθμο εκμάθησης θα τα ονομάζουμε χαρακτηριστικά εισόδου. Ακόμη, το $φ$ θα υποδηλώνει την αντιστοίχιση χαρακτηριστικών, η οποία αντιστοίχεί από τα γνωρίσματα στα χαρακτηριστικά. Αυτό γίνεται διότι μπορεί να θέλουμε να εφαρμόσουμε τον αλγόριθμο Μηχανών Διανυσμάτων Υποστήριξης ({\en{SVM}}) με κάποια χαρακτηριστικά $φ(x)$ αντί των αρχικών γνωρισμάτων. Αυτό επιτυγχάνεται βρίσκοντας το $φ(x)$ για κάθε $x$ στα στοιχεία εισόδου. 
	\par Αφού είδαμε ότι ο αλγόριθμος μπορεί να έχει τη μορφή όρων εσωτερικού γινομένου $<x,z>$, μπορούμε να τα αντικαταστήσουμε με $<φ(x),φ(z)>$. Δεδομένης μιας αντιστοίχισης $φ(x)$, ορίζουμε τον αντίστοιχο πυρήνα να είναι:\\
	$$K(x,z)=φ(x)^T φ(z)$$ 
	Έτσι όπου είχαμε πριν $<x,z>$ αντικαθιστούμε με $K(x,z)$ και τώρα ο αλγόριθμος μας μαθαίνει χρησιμοποιώντας τα χαρακτηριστικά $φ$.
	\par Έπειτα, δεδομένου $φ$ μπορούμε να βρούμε το $K(x,z)$ υπολογίζοντας το $φ(x)$ και το $φ(z)$ και έπειτα το εσωτερικό τους γινόμενο. Όμως, πολύ ενδιαφέρουσα είναι η παρατήρηση ότι πολλές φορές το $K(x,z)$ μπορεί να είναι υπερβολικά χρονοβόρο να υπολογιστεί, όπως και το $φ(x)$ (πιθανότατα διότι πρόκειται για έναν πολυδιάστατο πίνακα). Έτσι, εισάγοντας στον αλγόριθμο μας έναν αποτελεσματικό τρόπο να υπολογίζει το $K(x,z)$ μπορούμε να τον κάνουμε να επιτυγχάνει μάθηση σε μεγαλύτερες διαστάσεις, χωρίς να χρειαστεί ποτέ να υπολογίσει αυτές τις ποσότητες.
	\par Ας δούμε ένα παράδειγμα. Έστω $ x , z \in  R^n $ και:
				$$ K(x,z)=(x^T z)^2 $$
	Μπορούμε επίσης να το γράψουμε σαν:\\
	\begin{align}
	K(x,z)&=\left( \sum_{i=1}^n x_i z_i \right) \left(  \sum_{j=1}^n x_j z_j \right) \\
			  &=\sum_{i=1}^n \sum_{j=1}^n x_i z_i x_j z_j\\
			  &=\sum_{i=1}^n \sum_{j=1}^n ( x_i x_j)(z_i z_j)
	\end{align}
 
 Άρα μπορούμε να δούμε ότι το $K(x,z)=φ(x)^T φ(z)$, όπου η συνάρτηση χαρακτηριστικών $φ$ προκύπτει (για παράδειγμα στην περίπτωση που $n=3$) από:\\
 $$
 \left[
 \begin{matrix}
 	x_1 x_1\\
 	x_1 x_2\\
 	x_1 x_3\\
 	x_2 x_1\\
 	x_2 x_2\\
 	x_2 x_3\\
 	x_3 x_1\\
 	x_3 x_2\\
 	x_3 x_3\\
 \end{matrix}
 \right]
$$

Αξίζει να σημειωθεί ότι αν ο υπολογισμός της υψηλών διαστάσεων $φ(x)$ απαιτούσε χρόνο $O(n^2)$, ο υπολογισμός του $K(x,z)$ θα χρειαστεί μόλις $O(n)$ χρόνο.\\
 Για έναν σχετικό πυρήνα, θα έχω:\\
 	\begin{align*}
 		K(x,z) &= (x^T z+c)^2\\
 			   &=\sum_{i,j=1}^n (x_i x_j)(z_i z_j)+ \sum_{i,j=1}^n ( \sqrt{2c} x_i)( \sqrt{2c} z_i) +c^2
 	\end{align*}

Με αντίστοιχο πίνακα χαρακτηριστικών:\\
 $$
 \left[
 \begin{matrix}
 	x_1 x_1\\
 	x_1 x_2\\
 	x_1 x_3\\
 	x_2 x_1\\
 	x_2 x_2\\
 	x_2 x_3\\
 	x_3 x_1\\
 	x_3 x_2\\
 	x_3 x_3\\
 	\sqrt{2c}x_1\\
 	\sqrt{2c}x_2\\
 	\sqrt{2c}x_3\\
 	c
 \end{matrix}
 \right]
$$
 όπου η παράμετρος $c$ ελέγχει τα σχετικά βάρη μεταξύ των όρων $x_i$ (πρώτης τάξης) και $x_i x_j$ (δεύτερης τάξης).\\
 Γενικά ο πυρήνας $K(x,z)=(x^T z+c)$ αντιστοιχεί σε μία αντιστοίχιση χαρακτηριστικών σε ${n+d}\choose{d}$ χώρο χαρακτηριστικών, που αντιστοιχεί στα μονώνυμα της μορφής $x_{i1}, x_{i2},...,x_{ik}$ έως τάξης $d$. Ωστόσο, παρά το γεγονός ότι εργαζόμαστε σε αυτόν τον $O(n^d)$ διαστάσεων χώρο, ο υπολογισμός $K(x,z)$ εξακολουθεί να παίρνει μόνο $O(n)$ χρόνο και επομένως δεν χρειάζεται να εκφράσουμε τα διανύσματα χαρακτηριστικών σε αυτόν τον πολύ υψηλών διαστάσεων χώρο.\\
 \par Τώρα, ας δούμε μία λίγο πιο διαφορετική σκοπιά του πυρήνα. Διαισθητικά, αν τα $φ(x)$ και $φ(z)$ βρίσκονται κοντά το ένα στο άλλο, τότε θα περιμένουμε το $K(x,z)=φ(x)^T φ(z)$ να είναι μεγάλο. Αντίστοιχα για $φ(x)$, $φ(z)$ απομακρυσμένα το ένα από το άλλο θα πρέπει η αντίστοιχη ποσότητα $K(x,z)$ να είναι μικρή. Συνεπώς μπορούμε να δούμε το  $K(x,z)$ σαν ένα μέτρο σύγκρισης του πόσο κοντινά είναι τα $φ(x)$ και $φ(z)$, ή ακόμη και τα $z$,  $z$.\\
 Δεδομένου του παραπάνω συλλογισμού, έστω ότι σε ένα τυχαίο πρόβλημα εκμάθησης έχετε καταλήξει στην παρακάτω εξίσωση για την σύγκριση των $x$, $z$:\\
% $$K(x,z)=e^{\left( - \dfrac{\Vert x-z \Vert ^2}{2σ^2} \right)}$$
 \begin{align*}
 	K(x,z)=e^{\left( - \dfrac{\Vert x-z \Vert ^2}{2σ^2} \right)}
 \end{align*}
 Αποτελεί μία λογική συνάρτηση σύγκρισης μιας και το αποτέλεσμα πλησιάζει στο ένα αν τα $x$ και $z$ βρίσκονται κοντά, ενώ πλησιάζει το μηδέν αν τα $x$ και $z$ είναι μακριά. Μπορεί όμως αυτός ο ορισμός του $K$ να χρησιμοποιηθεί σαν πυρήνας σε αλγόριθμο Μηχανών Διανυσμάτων Υποστήριξης ({\en{SVM}}). Για τη συγκεκριμένη σχέση η απάντηση είναι θετική και μάλιστα πρόκειται για τον πυρήνα {\en{Gauss}}. Όμως με ποιο τρόπο μπορούμε γενικά να αποφανθούμε αν ένας πυρήνας είναι κατάλληλος;\\
 Υποθέτουμε αρχικά ότι ο $K$ είναι κατάλληλος πυρήνας που αντιστοιχεί σε αντιστοίχιση χαρακτηριστικών $φ$. Έστω ένα πεπερασμένο πλήθος $m$ σημείων ${x^1, x^2,..., x^m}$, και έστω ένας τετραγωνικός $m_x m$ πίνακας $K$ τέτοιος ώστε η είσοδος $(i,j)$ υπολογίζεται από τον τύπο $K_{ij}=K(x^i,x^j)$. Ο πίνακας αυτός καλείται πίνακας πυρήνα. Η απόφαση να ονομάσουμε $K$ και τον πίνακα και την συνάρτηση δεν αποτελεί πιθανό λάθος, αλλά είναι σκόπιμο λόγω της κοντινής τους σχέσης.\\
 \par Τώρα, έχοντας $K$ έναν αποδεκτό πυρήνα, $K_{ij}=K(x^i,x^j)=φ(x^i)^T φ(x^j)=φ(x^i) φ(x^j)^T=K(x^j,x^i)=K_{ji}$, και συνεπώς το $K$ πρέπει να είναι συμμετρικό. Έστω $φ_k(x)$ να είναι η κ-οστή συντεταγμένη του διανύσματος $φ(x)$. Για κάποιο διάνυσμα $z$ έχουμε:\\
 
 \begin{align}
 z^T K z &= \sum_i  \sum_j z_i K_{ij} z_j \\
 		 &= \sum_i \sum_j z_i φ(x^i)^T φ(x^j) z_j \\
 	&= \sum_i  \sum_j z_i \sum_k φ_k(x^i) φ_k(x^j)z_j\\
 	 &= \sum_k \sum_i \sum_j z_i φ_k(x^i) φ_k (x^j)z_j\\
 	   &=\sum_k \left(  \sum_i z_i φ_k(x^i) \right) ^ 2 	 
\end{align}  

 Συνεπώς, αφού το $z$ είναι αυθαίρετο, το $K$ θα είναι θετικό ($K \geq 0 $). \\
\par Μέχρι στιγμής έχουμε δείξει ότι εάν το $K$ είναι ένας έγκυρος πυρήνας, τότε ο αντίστοιχος πίνακας πυρήνα $K \in R ^ {mxm}$ είναι συμμετρικός και θετικά ορισμένος. Αυτό αποδεικνύεται όχι μόνο απαραίτητη, αλλά και επαρκής προϋπόθεση προκειμένου το $K$ να είναι ένας έγκυρος πυρήνας. Ο πυρήνας αυτός ονομάζεται και πυρήνας {\en{Mercer}}.

\begin{theo}
	Έστω $ K : R^n x R^n\rightarrow R $. Τότε προκειμένου το $K$ να είναι έγκυρος πυρήνας {\en{Mercer}} είναι αναγκαίο και επαρκές ότι για κάθε ${x_1, x_2,...,x_m: m < \infty}$, ο αντίστοιχος πίνακας πυρήνα είναι συμμετρικός και θετικά ορισμένος.
\end{theo}

\subsection{Κανονικοποίηση Και Περίπτωση Μη-Διαχωρισιμότητας}
Οι παραλλαγές των Μηχανών Διανυσμάτων Υποστήριξης ({\en{SVM}}) που έχουμε συναντήσει ως τώρα είχαν ως δεδομένο ότι το δείγμα είναι γραμμικά διαχωρίσιμο. Ενώ είναι πολύ πιθανό η αντιστοίχιση των στοιχείων σε έναν υψηλής διάστασης χώρο, διαμέσου της $φ$, να μας δώσει ένα αποτέλεσμα που διαχωρίζεται δεν μπορούμε να το εγγυηθούμε. Επίσης, σε ορισμένες περιπτώσεις δεν είναι ξεκάθαρο ότι η εύρεση διαχωριστικού υπερεπιπέδου είναι ακριβώς αυτό που θέλουμε να κάνουμε, καθώς αυτό μπορεί να είναι αναξιόπιστο σε υπερβολικά υψηλά επίπεδα. Για παράδειγμα, το σχήμα \ref{sx1} δείχνει ένα βέλτιστο όριο απόφασης, και αν προσθέσουμε ένα μόνο στοιχείο στο δείγμα μας, στην πάνω αριστερή περιοχή εικόνα \ref{sx2}, προκαλεί μια δραματική μεταβολή στο όριο απόφασης και ο νέος ταξινομητής έχει πολύ μικρότερο περιθώριο.

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{separate_1.eps}
  \caption{Διαχωρισμός 1}
  \label{sx1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{separate_2.eps}
  \caption{Διαχωρισμός 2}
  \label{sx2}
\end{minipage}
\end{figure}

\par Για να μπορέσει ο αλγόριθμος να επεξεργαστεί δεδομένα που δεν διαχωρίζονται γραμμικά, και να αποδίδει καλύτερα σε υψηλές διαστάσεις, μετατρέπουμε το υπάρχον πρόβλημα ως εξής:\\

$$min_{g,w,b} \dfrac{1}{2} \Vert w \Vert ^2 + c \sum_{i=1}^m g_i $$\\
τέτοια ώστε:\\
$$y^i(w^T x^i+b)\geq 1-g_i, i=1,...,m$$\\
$$g_i \leq 0, i=1,...,m$$

\par Με την αλλαγή αυτή το λειτουργικό περιθώριο μπορεί πια να λάβει τιμές μικρότερες του ένα. Η παράμετρος $c$ ελέγχει τα σχετικά βάρη ανάμεσα στους δύο μας στόχους που είναι η ελαχιστοποίηση του $\Vert w \Vert ^2 $ (το οποίο είδαμε ότι κάνει το περιθώριο μεγαλύτερο), και να εξασφαλίσει ότι τα περισσότερα στοιχεία στο δείγμα έχουν λειτουργικό περιθώριο τουλάχιστον ίσο με ένα. Όμοια με πριν, θα προκύψει και ο τύπος του {\en{Lagrange}}:\\
$$L(w,b,g,a,r)= \dfrac{1}{2} w^T w +c \sum_{i=1}^m g_i - \sum_{i=1}^m a_i[y^ {\left( i \right)} (x^T w + b)-1 + g_i ]- \sum_{i=1}^m r_i g_i$$

Εδώ, τα $a_i$ και $r_i$ είναι οι πολλαπλασιαστές {\en{Lagrange}} (οι οποίοι είναι μεγαλύτεροι η ίσοι με το μηδέν). Όμοια με πριν, αν παραγωγίσουμε ως προς $w$ και $b$ αντικαταστήσουμε και απλοποιήσουμε τη σχέση μας, θα προκύψει η διατύπωση:\\

$$max_a W(a)=\sum_{i=1}^m a_i - \dfrac{1}{2}\sum_{i,j=1}^m y^{\left(i \right)}y^{\left( j \right)}a_i a_j <x^{\left( i \right)}, x^{\left( j \right)}>$$ \\
τέτοια ώστε:\\
$$ 0 \leq a_i \leq c, i=1,...,m $$\\
$$ \sum_{i=1}^m a_i y^{ \left( i \right) } = 0 $$

Όπως αναφέραμε και παραπάνω τα $w$ μπορούν να εκφραστούν με βάση τα $a_i$ όπως στη σχέση $w=\sum_{i=1}^m a_i y^{\left( i \right)}x^{\left( i \right)}$. Μετά τη λύση τους διπλού προβλήματος μπορούμε να χρησιμοποιήσουμε τη σχέση $w^T x+b= \sum_{i=1}^m a_i y^{\left( i \right)} <x^{\left( i \right)}, x>+b$ για να εξάγουμε τις προβλέψεις μας. Η μόνη ουσιαστική αλλαγή που προέκυψε από την τροποποίηση του διπλού προβλήματος είναι ότι η συνθήκη $a_i \leq 0$ έγινε $0 \leq a_i \leq c$. Ακόμη πρέπει να τροποποιηθεί ο υπολογισμός του $b^*$. Τέλος οι συνθήκες {\en{KKT}} θα γίνουν:\\
$$ a_i=0 \Rightarrow y^{\left( i \right)}(w^T x^{\left( i \right)} +b) \geq 1 $$\\
$$ a_i=c \Rightarrow y^{\left( i \right)}(w^T x^{\left( i \right)} +b) \leq 1 $$\\
$$ 0<a_i<c \Rightarrow y^{\left( i \right)}(w^T x^{\left( i \right)} +b) = 1 $$\\
Τέλος, μένει να βρούμε έναν αλγόριθμο ικανό να λύσει αυτό το πρόβλημα. Έχοντας καταλήξει λοιπόν σε ένα δυικό πρόβλημα βελτιστοποίησης, υπάρχουν διάφοροι αλγόριθμοι για την επίλυση του. Ένας εξ αυτών είναι και ο αλγόριθμος {\en{Sequential Minimal Optimization (SMO)}}. Η επίλυση από αυτό το σημείο και έπειτα είναι καθαρά υπολογιστικό ζήτημα, συνεπώς η απόδειξη τελειώνει εδώ.
	\par Τέλος, θα αναφερθούμε σε κάποια ιστορικά στοιχεία σύμφωνα με την εξέλιξη του αλγορίθμου. Η αρχική ιδέα των Μηχανών Υποστήριξης Διανυσμάτων ({\en{SVM}}) επινοήθηκε από τους {\en{Vladimir N. Vapnik}} και {\en{ Alexey Ya. Chervonenkis}} το έτος 1963.  Το έτος 1992 οι {\en{Bernhard E. Boser, Isabelle M. Guyon}} και {\en{ Vladimir N. Vapnik}} πρότειναν έναν τρόπο δημιουργίας μη γραμμικών ταξινομητών εφαρμόζοντας το τέχνασμα του πυρήνα σε υπερβολικά περιθώρια μέγιστου περιθωρίου. Το τελικό στάδιο στο κομμάτι εξέλιξης των Μηχανών Υποστήριξης Διανυσμάτων  ({\en{SVM}}) προέκυψε έπειτα από την πρόταση των {\en{Corinna Cortes}} και {\en{Vapnik}} το 1993 η οποία εισήγαγε τα {\en{soft margins}} και η τελική δημοσίευση έγινε το 1995.

		\section{Αλγόριθμος K-Πλησιέστερων Γειτόνων}
		O αλγόριθμος $k$-πλησιέστερων γειτόνων {\en{(k-NN)}} είναι μια μη παραμετρική μέθοδος που χρησιμοποιείται για ταξινόμηση και παλινδρόμηση. Και στις δύο περιπτώσεις, η είσοδος αποτελείται από τα πλησιέστερα παραδείγματα εκπαίδευσης στο χώρο των χαρακτηριστικών. Η έξοδος εξαρτάται από το εάν ο αλγόριθμος χρησιμοποιείται για ταξινόμηση ή παλινδρόμηση:\\
		\begin{itemize}
		\item Στην ταξινόμηση η έξοδος είναι μέλος κάποιας τάξης. Ένα αντικείμενο ταξινομείται από μια πλειονότητα των ψήφων των γειτόνων του, με το αντικείμενο να ανατίθεται στην τάξη που είναι πιο συνηθισμένη στους $k$ πλησιέστερους γείτονές του (όπου $k$ είναι ένας θετικός ακέραιος, συνήθως μικρός). Σε περίπτωση που $k = 1$, τότε το αντικείμενο απλώς αποδίδεται στην κλάση του συγκεκριμένου πλησιέστερου γείτονα.
		\item Στην παλινδρόμηση, η έξοδος είναι η τιμή ιδιότητας για το αντικείμενο. Αυτή η τιμή είναι ο μέσος όρος των τιμών των πλησιέστερων γειτόνων.
		\end{itemize}
		\par Οι γείτονες λαμβάνονται από ένα σύνολο αντικειμένων για τα οποία είναι γνωστή η τάξη (για την ταξινόμηση) ή η τιμή ιδιότητας αντικειμένου (για παλινδρόμηση). Αυτό μπορεί να θεωρηθεί ως το δείγμα εκπαίδευσης για τον αλγόριθμο, αν και δεν απαιτείται ρητό εκπαιδευτικό βήμα.
		\par Τόσο για την ταξινόμηση όσο και για την παλινδρόμηση, μια χρήσιμη τεχνική μπορεί να χρησιμοποιηθεί για να αποδώσει το βάρος στις συνεισφορές των γειτόνων, έτσι ώστε οι πλησιέστεροι γείτονες να συμβάλλουν περισσότερο στον μέσο όρο από τους πιο μακρινούς. Για παράδειγμα, ένα κοινό μέσο βάρος μπορεί να αποδοθεί σε κάθε γείτονα με την τιμή $\dfrac{1}{d}$, όπου $d$ είναι η απόσταση από τον γείτονα.
		\par Το {\en{k-NN}} είναι ένας τύπος μάθησης βασισμένου σε παραδείγματα, όπου η λειτουργία προσεγγίζεται μόνο τοπικά και όλος ο υπολογισμός αναβάλλεται μέχρι την ταξινόμηση. Αποτελεί ένας από τους απλούστερους αλγορίθμους του συνόλου των αλγορίθμων μηχανικής μάθησης. 